{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_text = pd.read_json(\"https://bit.ly/nlp-tweet-train\",lines=True)\n",
    "test_text =pd.read_json(\"https://bit.ly/nlp-tweet-test\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "tokenizer = TweetTokenizer()\n",
    "def get_corpus(text):\n",
    "    cp =[]\n",
    "    for sentence in text['text']:\n",
    "        corpus =[token for token in re.split('\\W+',sentence.lower()) if token !='']\n",
    "        cp.append(corpus)\n",
    "    return cp\n",
    "\n",
    "#train_tokens = [tokenizer.tokenize(re.split('\\W+',sentence.lower()))for sentence in train_text['text'] ]\n",
    "#test_tokens = [tokenizer.tokenize(re.split('\\W+',sentence.lower()))for sentence in test_text['text'] ]\n",
    "train_tokens = get_corpus(train_text)\n",
    "test_tokens = get_corpus(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokens):\n",
    "    vocab = Counter()\n",
    "    for k in tokens:\n",
    "        vocab.update(k)\n",
    "    vocab = [token if freq>2 else \"<UNK>\" for token,freq in vocab.most_common()]\n",
    "    return vocab\n",
    "vocab_train = get_vocab(train_tokens)\n",
    "vocab_test = get_vocab(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(corpus_token,vocab):\n",
    "    corpus_bigrams=[]\n",
    "    tmp = corpus_token\n",
    "    for i in range(len(corpus_token)):\n",
    "        for j in range(len(corpus_token[i])):\n",
    "            if corpus_token[i][j] not in vocab:\n",
    "                #print(corpus_token[i][j])\n",
    "                tmp[i][j] = \"<UNK>\"\n",
    "        corpus_bigrams.append(list(nltk.bigrams(['<s>'] + tmp[i] +['</s>'])))\n",
    "    return corpus_bigrams\n",
    "train_bigram = get_bigram(train_tokens,vocab_train)\n",
    "test_bigram = get_bigram(test_tokens,vocab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(train_bigram):\n",
    "    counts = defaultdict(lambda: defaultdict(lambda:0))\n",
    "    for bi_tok in train_bigram:\n",
    "        for w1,w2 in bi_tok:\n",
    "        #print(w1,w2)\n",
    "            counts[w1][w2]+=1\n",
    "    return counts\n",
    "train_counts = get_dictionary(train_bigram)\n",
    "test_counts = get_dictionary(test_bigram)\n",
    "#[counts[w1][w2] for w1,w2 in sentence_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count =[[1+train_counts[w1][w2] for w1,w2 in bi_gram ]for bi_gram in train_bigram]\n",
    "train_counts_sum=[[len(vocab_train)+sum(train_counts[w1].values()) for w1,w2 in bi_gram ]for bi_gram in train_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used test_bigram\n",
    "test_count =[[1+test_counts[w1][w2] for w1,w2 in bi_gram ]for bi_gram in test_bigram]\n",
    "test_counts_sum=[[len(vocab_test)+sum(test_counts[w1].values()) for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used train_bigram\n",
    "test_count =[[1+train_counts[w1][w2] for w1,w2 in bi_gram ]for bi_gram in test_bigram]\n",
    "test_counts_sum=[[len(vocab_train)+sum(train_counts[w1].values()) for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPL(count,count_sum,bigram):\n",
    "    prob = [np.divide(x,y) for x,y in zip(count,count_sum)]\n",
    "    PP_list =[]\n",
    "    for i in range(len(prob)):\n",
    "        N = len(bigram[i])\n",
    "        tmp = np.prod(prob[i])\n",
    "        if tmp ==0:\n",
    "        #print('here')\n",
    "            tmp = 1e-200\n",
    "        a = math.pow(tmp,-1/N)\n",
    "        PP_list.append(a)\n",
    "    return np.mean(PP_list)\n",
    "print(\"train_AVG_PPL:{}\".format(PPL(train_count,train_counts_sum,train_bigram)))\n",
    "print(\"test_AVG_PPL:{}\".format(PPL(test_count,test_counts_sum,test_bigram)))\n",
    "#[sum(counts[w1].values()) for w1,w2 in sentence_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bi_dict(train_bigram):\n",
    "    counts_fw = defaultdict(lambda: defaultdict(lambda:0))\n",
    "    counts_bw =  defaultdict(lambda: defaultdict(lambda:0))\n",
    "    for bi_tok in train_bigram:\n",
    "        for w1,w2 in bi_tok:\n",
    "        #print(w1,w2)\n",
    "            counts_fw[w1][w2]+=1\n",
    "        for w1,w2 in bi_tok:\n",
    "            counts_bw[w2][w1]+=1\n",
    "    return counts_fw,counts_bw\n",
    "ct_fw,ct_bw = get_bi_dict(train_bigram)\n",
    "ctt_fw,ctt_bw = get_bi_dict(test_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts_fw =[[1+ct_fw[w1][w2] for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts_bw =[[1+ct_bw[w2][w1] for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 52, 1, 3, 109, 312, 1, 1, 1, 9705, 32, 4, 115, 2, 1, 171, 132, 109, 9, 4, 165] [107, 52, 1, 3, 109, 312, 1, 1, 1, 9705, 32, 4, 115, 2, 1, 171, 132, 109, 9, 4, 165]\n"
     ]
    }
   ],
   "source": [
    "print(test_counts_bw[0],test_counts_fw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fw_sum = [[(1 + ct_fw[bi_gram[i][0]][bi_gram[i][1]])/(len(vocab_train) + sum(ct_fw[bi_gram[i][0]].values())) for i in range(len(bi_gram)-1) ]for bi_gram in train_bigram]\n",
    "train_bw_sum = [[(1 + ct_bw[bi_gram[i][1]][bi_gram[i][0]])/(len(vocab_train) + sum(ct_bw[bi_gram[i][1]].values())) for i in range(1,len(bi_gram)) ]for bi_gram in train_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test used test_bigram\n",
    "#test_fw_sum=[[len(vocab)+sum(ct_fw[w1].values()) for w1,w2 in bi_gram ]for bi_gram in test_bigram]\n",
    "test_fw_sum = [[(1 + ctt_fw[bi_gram[i][0]][bi_gram[i][1]])/(len(vocab_test) + sum(ctt_fw[bi_gram[i][0]].values())) for i in range(len(bi_gram)-1) ]for bi_gram in test_bigram]\n",
    "test_bw_sum = [[(1 + ctt_bw[bi_gram[i][1]][bi_gram[i][0]])/(len(vocab_test) + sum(ctt_bw[bi_gram[i][1]].values())) for i in range(1,len(bi_gram)) ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used train_bigram\n",
    "test_fw_sum = [[(1 + ct_fw[bi_gram[i][0]][bi_gram[i][1]])/(len(vocab_train) + sum(ct_fw[bi_gram[i][0]].values())) for i in range(len(bi_gram)-1) ]for bi_gram in test_bigram]\n",
    "test_bw_sum = [[(1 + ct_bw[bi_gram[i][1]][bi_gram[i][0]])/(len(vocab_train) + sum(ct_bw[bi_gram[i][1]].values())) for i in range(1,len(bi_gram)) ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(fw,bw,gamma,bigram):\n",
    "    prob =[]\n",
    "    PP_list=[]\n",
    "    #gamma =np.arange(0,1,0.05)\n",
    "    for i in range(len(bigram)):\n",
    "        N = len(bigram[i])\n",
    "        #print(fw[i],gamma,len(bw[i]),len(test_sum[i]))\n",
    "        tmp = np.prod(list(map(lambda x,y:x*gamma+y*(1-gamma),fw[i],bw[i])))\n",
    "        #tmp = np.prod(np.divide(list(map(lambda x,y: x * gamma+y*(1-gamma), fw[i],bw[i])),test_sum[i]))\n",
    "        #print(tmp)\n",
    "        if tmp ==0:\n",
    "        #print('here')\n",
    "            tmp = 1e-200\n",
    "        a = math.pow(tmp,-1/N)\n",
    "        PP_list.append(a)\n",
    "        #prob.append(np.divide(fw[i]*gamma+bw[i]*(1-gamma),test_sum[i]))\n",
    "    return np.mean(PP_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2525.6197933675326\n",
      "0.0\n",
      "2030.9389852457593\n",
      "0.05\n",
      "1867.918942934626\n",
      "0.1\n",
      "1768.6487733650297\n",
      "0.15\n",
      "1700.1278816735107\n",
      "0.2\n",
      "1650.273794963705\n",
      "0.25\n",
      "1613.2807572861993\n",
      "0.3\n",
      "1585.9551232277859\n",
      "0.35\n",
      "1566.414492964196\n",
      "0.4\n",
      "1553.5317129130385\n",
      "0.45\n",
      "1546.6727647164112\n",
      "0.5\n",
      "1545.5732151249833\n",
      "0.55\n",
      "1550.2947989234303\n",
      "0.6\n",
      "1561.244771563732\n",
      "0.65\n",
      "1579.2680511245112\n",
      "0.7\n",
      "1605.8586674635587\n",
      "0.75\n",
      "1643.6196019546883\n",
      "0.8\n",
      "1697.3507963698917\n",
      "0.85\n",
      "1777.1421783228243\n",
      "0.9\n",
      "1910.9146876538598\n",
      "0.95\n",
      "2348.3541019291747\n",
      "1.0\n",
      "2817.2247230517305\n",
      "0.0\n",
      "2155.8666815049687\n",
      "0.05\n",
      "1957.5534521783004\n",
      "0.1\n",
      "1839.4153762791261\n",
      "0.15\n",
      "1759.0246437777466\n",
      "0.2\n",
      "1701.1826552228345\n",
      "0.25\n",
      "1658.6894324031596\n",
      "0.3\n",
      "1627.6269933122865\n",
      "0.35\n",
      "1605.7070603255365\n",
      "0.4\n",
      "1591.5719644765154\n",
      "0.45\n",
      "1584.467922766488\n",
      "0.5\n",
      "1584.0925472450078\n",
      "0.55\n",
      "1590.5427217395531\n",
      "0.6\n",
      "1604.3413538577631\n",
      "0.65\n",
      "1626.5565609974797\n",
      "0.7\n",
      "1659.0741998895235\n",
      "0.75\n",
      "1705.1930821929004\n",
      "0.8\n",
      "1771.0448218091683\n",
      "0.85\n",
      "1869.678334428886\n",
      "0.9\n",
      "2037.9290736209516\n",
      "0.95\n",
      "2642.629004041137\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def get_gamma(fw,bw,bigram):\n",
    "    gammas = np.arange(0,1.05,0.05)\n",
    "    tmp = math.pow(10,15)\n",
    "    tmp_g = 0\n",
    "    for gamma in gammas:\n",
    "        if tmp>get_ppl(fw,bw,gamma,bigram):\n",
    "            tmp = get_ppl(fw,bw,gamma,bigram)\n",
    "            tmp_g = gamma\n",
    "        #tmp_small = get_ppl\n",
    "\n",
    "        print(get_ppl(fw,bw,gamma,bigram))\n",
    "        print(int(gamma*100)/100)\n",
    "        #return tmp,tmp_g\n",
    "    return tmp,tmp_g\n",
    "best_train,best_train_gamma = get_gamma(train_fw_sum,train_bw_sum,train_bigram)\n",
    "best_test,best_test_gamma = get_gamma(test_fw_sum,test_bw_sum,test_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_data set best_of bidirection_ppl:1545.5732151249833,and it's gamma:0.55\n",
      "In test_data  set best_of bidirection_ppl:1584.0925472450078,and it's gamma:0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"In train_data set best_of bidirection_ppl:{},and it's gamma:{}\".format(best_train,best_train_gamma))\n",
    "print(\"In test_data  set best_of bidirection_ppl:{},and it's gamma:{}\".format(best_test,best_test_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
       "       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0,1.05,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]+[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n"
     ]
    }
   ],
   "source": [
    "print(len(test_counts_bw[0]),len(test_counts_fw[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 52, 1, 3, 109, 312, 1, 1, 1, 9705, 32, 4, 115, 2, 1, 171, 132, 109, 9, 4, 165]\n",
      "('your', 'spending')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_counts_bw[0])\n",
    "print(test_bigram[0][-8])\n",
    "print(ct_bw['</s>']['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'more'), ('more', 'money'), ('money', 'wasted'), ('wasted', 'on'), ('on', 'an'), ('an', '<UNK>'), ('<UNK>', 'antiquated'), ('antiquated', 'airport'), ('airport', 'it'), ('it', 's'), ('s', 'easy'), ('easy', 'when'), ('when', 'your'), ('your', 'spending'), ('spending', 'other'), ('other', 'people'), ('people', 's'), ('s', 'hard'), ('hard', 'earned'), ('earned', 'money'), ('money', '</s>')]\n",
      "more <s>\n",
      "106\n",
      "____________\n",
      "106\n",
      "money more\n",
      "51\n",
      "____________\n",
      "51\n",
      "wasted money\n",
      "0\n",
      "____________\n",
      "0\n",
      "on wasted\n",
      "2\n",
      "____________\n",
      "2\n",
      "an on\n",
      "108\n",
      "____________\n",
      "108\n",
      "<UNK> an\n",
      "311\n",
      "____________\n",
      "311\n",
      "antiquated <UNK>\n",
      "0\n",
      "____________\n",
      "0\n",
      "airport antiquated\n",
      "0\n",
      "____________\n",
      "0\n",
      "it airport\n",
      "0\n",
      "____________\n",
      "0\n",
      "s it\n",
      "9704\n",
      "____________\n",
      "9704\n",
      "easy s\n",
      "31\n",
      "____________\n",
      "31\n",
      "when easy\n",
      "3\n",
      "____________\n",
      "3\n",
      "your when\n",
      "114\n",
      "____________\n",
      "114\n",
      "spending your\n",
      "1\n",
      "____________\n",
      "1\n",
      "other spending\n",
      "0\n",
      "____________\n",
      "0\n",
      "people other\n",
      "170\n",
      "____________\n",
      "170\n",
      "s people\n",
      "131\n",
      "____________\n",
      "131\n",
      "hard s\n",
      "108\n",
      "____________\n",
      "108\n",
      "earned hard\n",
      "8\n",
      "____________\n",
      "8\n",
      "money earned\n",
      "3\n",
      "____________\n",
      "3\n",
      "</s> money\n",
      "164\n",
      "____________\n",
      "164\n",
      "[107, 52, 1, 3, 109, 312, 1, 1, 1, 9705, 32, 4, 115, 2, 1, 171, 132, 109, 9, 4, 165]\n",
      "[('<s>', 'ohhh'), ('ohhh', '<UNK>'), ('<UNK>', '<UNK>'), ('<UNK>', '<UNK>'), ('<UNK>', 'come'), ('come', 'dice'), ('dice', 'la'), ('la', '<UNK>'), ('<UNK>', 'forever'), ('forever', 'young'), ('young', '<UNK>'), ('<UNK>', 'young'), ('young', '</s>')]\n",
      "ohhh <s>\n",
      "33\n",
      "____________\n",
      "33\n",
      "<UNK> ohhh\n",
      "3\n",
      "____________\n",
      "3\n",
      "<UNK> <UNK>\n",
      "6772\n",
      "____________\n",
      "6772\n",
      "<UNK> <UNK>\n",
      "6772\n",
      "____________\n",
      "6772\n",
      "come <UNK>\n",
      "27\n",
      "____________\n",
      "27\n",
      "dice come\n",
      "0\n",
      "____________\n",
      "0\n",
      "la dice\n",
      "0\n",
      "____________\n",
      "0\n",
      "<UNK> la\n",
      "50\n",
      "____________\n",
      "50\n",
      "forever <UNK>\n",
      "10\n",
      "____________\n",
      "10\n",
      "young forever\n",
      "0\n",
      "____________\n",
      "0\n",
      "<UNK> young\n",
      "22\n",
      "____________\n",
      "22\n",
      "young <UNK>\n",
      "6\n",
      "____________\n",
      "6\n",
      "</s> young\n",
      "19\n",
      "____________\n",
      "19\n",
      "[34, 4, 6773, 6773, 28, 1, 1, 51, 11, 1, 23, 7, 20]\n",
      "[107, 52, 1, 3, 109, 312, 1, 1, 1, 9705, 32, 4, 115, 2, 1, 171, 132, 109, 9, 4, 165]\n"
     ]
    }
   ],
   "source": [
    "rs=[]\n",
    "for bi_gram in test_bigram[:2]:\n",
    "    ct=[]\n",
    "    print(bi_gram)\n",
    "    for i in range(len(bi_gram)):\n",
    "        print(bi_gram[i][1],bi_gram[i][0])\n",
    "        print(ct_bw[bi_gram[i][1]][bi_gram[i][0]])\n",
    "        print(\"____________\")\n",
    "        print(ct_fw[bi_gram[i][0]][bi_gram[i][1]])\n",
    "        ct.append(1+ct_bw[bi_gram[i][1]][bi_gram[i][0]])\n",
    "    print(ct)\n",
    "    rs.append(ct)\n",
    "print(rs[0])\n",
    "#test used really not used 無用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
