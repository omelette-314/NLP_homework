{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_text = pd.read_json(\"https://bit.ly/nlp-tweet-train\",lines=True)\n",
    "test_text =pd.read_json(\"https://bit.ly/nlp-tweet-test\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "tokenizer = TweetTokenizer()\n",
    "def get_corpus(text):\n",
    "    cp =[]\n",
    "    for sentence in text['text']:\n",
    "        corpus =[token for token in re.split('\\W+',sentence.lower()) if token !='']\n",
    "        cp.append(corpus)\n",
    "    return cp\n",
    "\n",
    "#train_tokens = [tokenizer.tokenize(re.split('\\W+',sentence.lower()))for sentence in train_text['text'] ]\n",
    "#test_tokens = [tokenizer.tokenize(re.split('\\W+',sentence.lower()))for sentence in test_text['text'] ]\n",
    "train_tokens = get_corpus(train_text)\n",
    "test_tokens = get_corpus(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokens):\n",
    "    vocab = Counter()\n",
    "    for k in tokens:\n",
    "        vocab.update(k)\n",
    "    vocab = [token if freq>2 else \"<UNK>\" for token,freq in vocab.most_common()]\n",
    "    return vocab\n",
    "vocab_train = get_vocab(train_tokens)\n",
    "vocab_test = get_vocab(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(corpus_token,vocab):\n",
    "    corpus_bigrams=[]\n",
    "    tmp = corpus_token\n",
    "    for i in range(len(corpus_token)):\n",
    "        for j in range(len(corpus_token[i])):\n",
    "            if corpus_token[i][j] not in vocab:\n",
    "                #print(corpus_token[i][j])\n",
    "                tmp[i][j] = \"<UNK>\"\n",
    "        corpus_bigrams.append(list(nltk.bigrams(['<s>'] + tmp[i] +['</s>'])))\n",
    "    return corpus_bigrams\n",
    "train_bigram = get_bigram(train_tokens,vocab_train)\n",
    "test_bigram = get_bigram(test_tokens,vocab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(train_bigram):\n",
    "    counts = defaultdict(lambda: defaultdict(lambda:0))\n",
    "    for bi_tok in train_bigram:\n",
    "        for w1,w2 in bi_tok:\n",
    "        #print(w1,w2)\n",
    "            counts[w1][w2]+=1\n",
    "    return counts\n",
    "train_counts = get_dictionary(train_bigram)\n",
    "test_counts = get_dictionary(test_bigram)\n",
    "#[counts[w1][w2] for w1,w2 in sentence_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count =[[1+train_counts[w1][w2] for w1,w2 in bi_gram ]for bi_gram in train_bigram]\n",
    "train_counts_sum=[[len(vocab_train)+sum(train_counts[w1].values()) for w1,w2 in bi_gram ]for bi_gram in train_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count =[[1+test_counts[w1][w2] for w1,w2 in bi_gram ]for bi_gram in test_bigram]\n",
    "test_counts_sum=[[len(vocab_test)+sum(test_counts[w1].values()) for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_AVG_PPL:3776.930335383124\n",
      "test_AVG_PPL:3056.7893683035227\n"
     ]
    }
   ],
   "source": [
    "def PPL(count,count_sum,bigram):\n",
    "    prob = [np.divide(x,y) for x,y in zip(count,count_sum)]\n",
    "    PP_list =[]\n",
    "    for i in range(len(prob)):\n",
    "        N = len(bigram[i])\n",
    "        tmp = np.prod(prob[i])\n",
    "        if tmp ==0:\n",
    "        #print('here')\n",
    "            tmp = 1e-200\n",
    "        a = math.pow(tmp,-1/N)\n",
    "        PP_list.append(a)\n",
    "    return np.mean(PP_list)\n",
    "print(\"train_AVG_PPL:{}\".format(PPL(train_count,train_counts_sum,train_bigram)))\n",
    "print(\"test_AVG_PPL:{}\".format(PPL(test_count,test_counts_sum,test_bigram)))\n",
    "#[sum(counts[w1].values()) for w1,w2 in sentence_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bi_dict(train_bigram):\n",
    "    counts_fw = defaultdict(lambda: defaultdict(lambda:0))\n",
    "    counts_bw =  defaultdict(lambda: defaultdict(lambda:0))\n",
    "    for bi_tok in train_bigram:\n",
    "        for w1,w2 in bi_tok:\n",
    "        #print(w1,w2)\n",
    "            counts_fw[w1][w2]+=1\n",
    "        for w1,w2 in bi_tok:\n",
    "            counts_bw[w2][w1]+=1\n",
    "    return counts_fw,counts_bw\n",
    "ct_fw,ct_bw = get_bi_dict(train_bigram)\n",
    "ctt_fw,ctt_bw = get_bi_dict(test_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts_fw =[[1+ct_fw[w1][w2] for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts_bw =[[1+ct_bw[w2][w1] for w1,w2 in bi_gram ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_counts_bw[0],test_counts_fw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fw_sum = [[(1 + ct_fw[bi_gram[i][0]][bi_gram[i][1]])/(len(vocab_train) + sum(ct_fw[bi_gram[i][0]].values())) for i in range(len(bi_gram)-1) ]for bi_gram in train_bigram]\n",
    "train_bw_sum = [[(1 + ct_bw[bi_gram[i][1]][bi_gram[i][0]])/(len(vocab_train) + sum(ct_bw[bi_gram[i][1]].values())) for i in range(1,len(bi_gram)) ]for bi_gram in train_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_fw_sum=[[len(vocab)+sum(ct_fw[w1].values()) for w1,w2 in bi_gram ]for bi_gram in test_bigram]\n",
    "test_fw_sum = [[(1 + ctt_fw[bi_gram[i][0]][bi_gram[i][1]])/(len(vocab_test) + sum(ctt_fw[bi_gram[i][0]].values())) for i in range(len(bi_gram)-1) ]for bi_gram in test_bigram]\n",
    "test_bw_sum = [[(1 + ctt_bw[bi_gram[i][1]][bi_gram[i][0]])/(len(vocab_test) + sum(ctt_bw[bi_gram[i][1]].values())) for i in range(1,len(bi_gram)) ]for bi_gram in test_bigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(fw,bw,gamma,bigram):\n",
    "    prob =[]\n",
    "    PP_list=[]\n",
    "    #gamma =np.arange(0,1,0.05)\n",
    "    for i in range(len(bigram)):\n",
    "        N = len(bigram[i])\n",
    "        #print(fw[i],gamma,len(bw[i]),len(test_sum[i]))\n",
    "        tmp = np.prod(list(map(lambda x,y:x*gamma+y*(1-gamma),fw[i],bw[i])))\n",
    "        #tmp = np.prod(np.divide(list(map(lambda x,y: x * gamma+y*(1-gamma), fw[i],bw[i])),test_sum[i]))\n",
    "        #print(tmp)\n",
    "        if tmp ==0:\n",
    "        #print('here')\n",
    "            tmp = 1e-200\n",
    "        a = math.pow(tmp,-1/N)\n",
    "        PP_list.append(a)\n",
    "        #prob.append(np.divide(fw[i]*gamma+bw[i]*(1-gamma),test_sum[i]))\n",
    "    return np.mean(PP_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma(fw,bw,bigram):\n",
    "    gammas = np.arange(0,1.05,0.05)\n",
    "    tmp = math.pow(10,15)\n",
    "    tmp_g = 0\n",
    "    for gamma in gammas:\n",
    "        if tmp>get_ppl(fw,bw,gamma,bigram):\n",
    "            tmp = get_ppl(fw,bw,gamma,bigram)\n",
    "            tmp_g = gamma\n",
    "        #tmp_small = get_ppl\n",
    "\n",
    "        print(get_ppl(fw,bw,gamma,bigram))\n",
    "        print(int(gamma*100)/100)\n",
    "        #return tmp,tmp_g\n",
    "    return tmp,tmp_g\n",
    "best_train,best_train_gamma = get_gamma(train_fw_sum,train_bw_sum,train_bigram)\n",
    "best_test,best_test_gamma = get_gamma(test_fw_sum,test_bw_sum,test_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In train_data set best_of bidirection_ppl:{},and it's gamma:{}\".format(best_train,best_train_gamma))\n",
    "print(\"In test_data  set best_of bidirection_ppl:{},and it's gamma:{}\".format(best_test,best_test_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,1.05,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]+[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_counts_bw[0]),len(test_counts_fw[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_counts_bw[0])\n",
    "print(test_bigram[0][-8])\n",
    "print(ct_bw['</s>']['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs=[]\n",
    "for bi_gram in test_bigram[:2]:\n",
    "    ct=[]\n",
    "    print(bi_gram)\n",
    "    for i in range(len(bi_gram)):\n",
    "        print(bi_gram[i][1],bi_gram[i][0])\n",
    "        print(ct_bw[bi_gram[i][1]][bi_gram[i][0]])\n",
    "        print(\"____________\")\n",
    "        print(ct_fw[bi_gram[i][0]][bi_gram[i][1]])\n",
    "        ct.append(1+ct_bw[bi_gram[i][1]][bi_gram[i][0]])\n",
    "    print(ct)\n",
    "    rs.append(ct)\n",
    "print(rs[0])\n",
    "#test used really not used 無用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
